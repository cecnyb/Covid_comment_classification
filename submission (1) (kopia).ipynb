{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ubo4LtV2hKG5"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import requests\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.calibration import LinearSVC\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
        "from sklearn.linear_model import Perceptron, LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8UmUapVhKG_"
      },
      "source": [
        "*A remark about this notebook:* Because we used computing resources of the RWTH Aachen High Performance Computing Cluster, we did not use a single coherent notebook. Instead, we used scripts to submit jobs to the cluster, and then collected all methods and results in this notebook. This is why the notebook is not as coherent as it could be."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snVGYhJVhKHH"
      },
      "source": [
        "### Convenience functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XkROz-CLhKHK"
      },
      "outputs": [],
      "source": [
        "# This is only required for running the hyperparameter search on the HPC cluster\n",
        "def parse_arguments():\n",
        "    parser = argparse.ArgumentParser(description=\"finding hyperparameters\")\n",
        "    # model_id = SLURM_ARRAY_TASK_ID\n",
        "    parser.add_argument(\"-i\", \"--model_id\", help=\"ID specifying a model\", type=int)\n",
        "    return parser.parse_args()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vs_9vuOuhKHK"
      },
      "outputs": [],
      "source": [
        "def save_model(model, fname):\n",
        "  with open(fname,'wb') as f:\n",
        "    pickle.dump(model, f)\n",
        "\n",
        "def load_model(fname):\n",
        "  with open(fname, 'rb') as f:\n",
        "    return pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D01LKYqxhKHK"
      },
      "outputs": [],
      "source": [
        "# Datasets\n",
        "TRAIN_SET = Path(\"./a3_train_final.tsv\")\n",
        "TEST_SET = Path(\"./a3_test.tsv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7gyos1znhKHK"
      },
      "outputs": [],
      "source": [
        "# Telegram settings, used to retrieve real-time updates from the cluster\n",
        "TELEGRAM_BOT_TOKEN = \"1234\"\n",
        "TELEGRAM_CHAT_ID = \"1234\"\n",
        "\n",
        "def send_telegram_message(text):\n",
        "    requests.get(f\"https://api.telegram.org/bot{TELEGRAM_BOT_TOKEN}/sendMessage?chat_id={TELEGRAM_CHAT_ID}&text={text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJmkdiJuhKHL"
      },
      "source": [
        "### Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rQmuahokhKHS"
      },
      "outputs": [],
      "source": [
        "label_dict = {\n",
        "    0: \"anti-vaccination\",\n",
        "    1: \"pro-vaccination\"\n",
        "}\n",
        "\n",
        "def remove_ambiguous_annotations(df):\n",
        "  def check_ambiguity(annotation):\n",
        "    individual_annotations = [int(i) for i in annotation.split(\"/\")]\n",
        "    # check if all annotations are equal\n",
        "    if individual_annotations.count(individual_annotations[0]) == len(individual_annotations):\n",
        "      return individual_annotations[0] # if yes, keep whatever that unanimous annotation was\n",
        "    else:\n",
        "      return -1 # if no, indicate an ambiguous annotation\n",
        "\n",
        "  # apply ambiguity check to every element\n",
        "  df[\"Annotation\"] = df[\"Annotation\"].apply(check_ambiguity)\n",
        "\n",
        "  # calculate annotator accuracy\n",
        "  unanimous_annotations = df[df[\"Annotation\"] != -1]\n",
        "  annotator_accuracy = len(unanimous_annotations) / len(df[\"Annotation\"])\n",
        "  print(f'Annotator agreement: {annotator_accuracy:.3%}')\n",
        "\n",
        "  # drop all ambiguous annotations\n",
        "  df.drop(df[df[\"Annotation\"] == -1].index, inplace=True)\n",
        "  return df\n",
        "\n",
        "def data_cleanser(df, col):\n",
        "    df[col] = df[col].replace('[^A-Za-z\\'\\s]+', ' ', regex=True)\n",
        "    return df\n",
        "\n",
        "def get_model_pipeline(model):\n",
        "    return Pipeline(steps = [(\"tfidf\", TfidfVectorizer()), (\"clf\", model)])\n",
        "\n",
        "def training_pipeline(X, Y, model, parameters):\n",
        "  pipeline = get_model_pipeline(model)\n",
        "  randomizedSearchCV = RandomizedSearchCV(pipeline, param_distributions=parameters, n_jobs=-1, random_state=0)\n",
        "  randomizedSearchCV.fit(X,Y)\n",
        "  return randomizedSearchCV.best_estimator_, randomizedSearchCV.best_params_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZuvJSqghKHT"
      },
      "source": [
        "### Hyperparameter dictionaries\n",
        "The hyperparameter dictionaries are used to store the hyperparameter distributions for the models. The hyperparameters are sampled from these distributions to create the models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kW4Hv8vrhKHT"
      },
      "outputs": [],
      "source": [
        "## Vectorizer hyperparameters\n",
        "parameters_tfidf = {\n",
        "    # Decide on whether to limit the maximum number of features\n",
        "    \"tfidf__max_features\": [None, 1000],\n",
        "\n",
        "    # Decide wether the feature should be made of word or character n-grams\n",
        "    \"tfidf__analyzer\": [\"word\", \"char\"],\n",
        "\n",
        "    # Smooth idf weights or not\n",
        "    \"tfidf__smooth_idf\": [True, False],\n",
        "\n",
        "    # Upper boundary of n-values for different n-grams, here: unigrams and bigrams\n",
        "    \"tfidf__ngram_range\": [(1, 1), (1, 2)],\n",
        "\n",
        "    # Enable inverse-document-frequency reweighting. or not\n",
        "    \"tfidf__use_idf\": [True, False],\n",
        "\n",
        "    # Decide wether to remove stop words like \"and\", \"the\", \"him\"...\n",
        "    \"tfidf__stop_words\": [None, list(STOP_WORDS)]\n",
        "}\n",
        "\n",
        "## Model hyperparameters\n",
        "parameters_dc = {}\n",
        "\n",
        "parameters_random_forest = {\n",
        "# Number of trees in random forest\n",
        "'clf__n_estimators': [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)],\n",
        "\n",
        "# Maximum number of levels in tree\n",
        "'clf__max_features': ['auto', 'sqrt', 'log2', None],\n",
        "\n",
        "# Maximum number of levels in tree\n",
        "'clf__max_depth': [int(x) for x in np.linspace(10, 110, num = 11)],\n",
        "\n",
        "# Minimum number of samples required to split a node\n",
        "'clf__min_samples_split': [2, 5, 10],\n",
        "\n",
        "# Minimum number of samples required at each leaf node\n",
        "'clf__min_samples_leaf': [1, 2, 4],\n",
        "\n",
        "# With or without replacement\n",
        "'clf__bootstrap': [True, False]\n",
        "}\n",
        "\n",
        "parameters_svc = {\n",
        "  # Determine loss function\n",
        "  'clf__loss': [\"hinge\", \"squared_hinge\"],\n",
        "\n",
        "  # Regularization parameter - regularization i inversely proportional to C\n",
        "  'clf__C': [0.1, 1, 10, 100, 1000],\n",
        "\n",
        "  # Tolerance for stopping criteria\n",
        "  'clf__tol': [1e-3,1e-4,1e-5]\n",
        "}\n",
        "\n",
        "parameters_knn = {\n",
        "  # Determine number of neighbors\n",
        "  'clf__n_neighbors': list(range(1,31)),\n",
        "\n",
        "  # Determine weight function used in prediction\n",
        "  'clf__weights': [\"uniform\", \"distance\"],\n",
        "\n",
        "  # Determine algorithm used to compute the nearest neighbors\n",
        "  'clf__algorithm': [\"auto\", \"ball_tree\", \"kd_tree\", \"brute\"]\n",
        "}\n",
        "\n",
        "parameters_naive_b = {\n",
        "    # Decide wether to learn class prior probabilities or not\n",
        "    'clf__fit_prior': (True, False),\n",
        "\n",
        "    # Additive smooting paramter\n",
        "    'clf__alpha': (0.5, 1.0)\n",
        "}\n",
        "\n",
        "parameters_perceptron = {\n",
        "    # Determine the learning rate\n",
        "    'clf__eta0': (0.1, 0.01, 0.001),\n",
        "\n",
        "    # Determine the learning rate\n",
        "    'clf__penalty': [\"l2\", \"l1\", \"elasticnet\"],\n",
        "\n",
        "    # Determine the learning rate\n",
        "    'clf__alpha': (0.0001, 0.00001, 0.000001)\n",
        "}\n",
        "\n",
        "parameters_decision_tree = {\n",
        "    # Maximum number of levels in tree\n",
        "    'clf__max_depth': [int(x) for x in np.linspace(10, 110, num = 11)],\n",
        "\n",
        "    # Minimum number of samples required to split a node\n",
        "    'clf__min_samples_split': [2, 5, 10],\n",
        "\n",
        "    # Minimum number of samples required at each leaf node\n",
        "    'clf__min_samples_leaf': [1, 2, 4],\n",
        "\n",
        "    # Maximum number of levels in tree\n",
        "    'clf__max_features': ['auto', 'sqrt', 'log2', None]\n",
        "}\n",
        "\n",
        "parameters_logistic_regression = {\n",
        "    # Regularization parameter\n",
        "    'clf__C': [0.1, 1, 10, 100, 1000],\n",
        "\n",
        "    # Regularization parameter\n",
        "    'clf__penalty': [\"l2\", \"l1\", \"elasticnet\"],\n",
        "\n",
        "    # Regularization parameter\n",
        "    'clf__solver': [\"newton-cg\", \"lbfgs\", \"liblinear\", \"sag\", \"saga\"]\n",
        "}\n",
        "\n",
        "parameters_gradient_boosting = {\n",
        "    # Number of boosting stages\n",
        "    'clf__n_estimators': [100, 200, 300, 400, 500],\n",
        "\n",
        "    # Maximum depth of the individual estimators\n",
        "    'clf__max_depth': [3, 4, 5, 6, 7],\n",
        "\n",
        "    # Learning rate shrinks the contribution of each tree\n",
        "    'clf__learning_rate': [0.1, 0.01, 0.001],\n",
        "\n",
        "    # Subsample ratio of the training instance\n",
        "    'clf__subsample': [0.7, 0.8, 0.9, 1.0],\n",
        "\n",
        "    # Maximum number of features to consider for making splits\n",
        "    'clf__max_features': ['auto', 'sqrt', 'log2', None]\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WdNHEynkhKHV"
      },
      "outputs": [],
      "source": [
        "# Mapping models to their parameter distribution\n",
        "models_param_dict = {\n",
        "    DummyClassifier(): parameters_dc,\n",
        "    GradientBoostingClassifier(): parameters_gradient_boosting,\n",
        "    RandomForestClassifier(): parameters_random_forest,\n",
        "    Perceptron(): parameters_perceptron,\n",
        "    DecisionTreeClassifier(): parameters_decision_tree,\n",
        "    LogisticRegression(): parameters_logistic_regression,\n",
        "    MultinomialNB(): parameters_naive_b,\n",
        "    LinearSVC(): parameters_svc,\n",
        "    KNeighborsClassifier(): parameters_knn\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usQeknhfhKHV"
      },
      "source": [
        "### Preprocessing\n",
        "The preprocessing functions are used to preprocess the data before it is used to train the models. We have separate preprocessing for the TfIdf and the Sentence Transformer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jhiNAbmFhKHV"
      },
      "outputs": [],
      "source": [
        "# Preprocessing for TfIdf\n",
        "train_data = pd.read_csv(TRAIN_SET, sep='\\t', header = None)\n",
        "train_data.columns = [\"Annotation\", \"Comment\"]\n",
        "remove_ambiguous_annotations(train_data)\n",
        "print(f\"{len(train_data)} training samples remain.\")\n",
        "data_cleanser(train_data, \"Comment\")\n",
        "data_shuffled = train_data.sample(frac=1.0, random_state=0)\n",
        "\n",
        "Xtrain = data_shuffled.iloc[:, 1]\n",
        "Ytrain = data_shuffled.iloc[:, 0]\n",
        "\n",
        "\n",
        "test_data = pd.read_csv(TEST_SET, sep='\\t', header=None)\n",
        "test_data.columns = [\"Annotation\", \"Comment\"]\n",
        "print(f\"{len(test_data)} test samples.\")\n",
        "data_cleanser(test_data, \"Comment\")\n",
        "data_shuffled_test = test_data.sample(frac=1.0, random_state=0)\n",
        "\n",
        "Xtest = data_shuffled_test.iloc[:, 1]\n",
        "Ytest = data_shuffled_test.iloc[:, 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-v2kARQhKHX"
      },
      "outputs": [],
      "source": [
        "# Preprocessing for Sentence Transformer\n",
        "\n",
        "# The encoding needs to be run just once and can be stored using pickle\n",
        "# This is very handy, because the encoding takes quite some time\n",
        "\n",
        "# This is just to make it run on the GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = SentenceTransformer(\"all-mpnet-base-v2\").to(device)\n",
        "x_train_emb = model.encode(list(Xtrain))\n",
        "x_test_emb = model.encode(list(Xtest))\n",
        "with open(\"xtest_embd.pkl\",'wb') as f:\n",
        "    pickle.dump(x_test_emb, f)\n",
        "with open(\"xtrain_embd.pkl\",'wb') as f:\n",
        "    pickle.dump(x_train_emb, f)\n",
        "\n",
        "# Afterwards, we can just load the serialized embedding, which is a lot faster\n",
        "x_train_emb = load_model(\"xtrain_embd.pkl\")\n",
        "x_test_emb = load_model(\"xtest_embd.pkl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mRMeAF4hKHX"
      },
      "source": [
        "### Model evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P5RjSyR1hKHZ"
      },
      "outputs": [],
      "source": [
        "# First: fit each model without tuning any parameters\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_train_tfidf = vectorizer.fit_transform(Xtrain)\n",
        "X_test_tfidf = vectorizer.transform(Xtest)\n",
        "\n",
        "result_accuracy = {}\n",
        "for model in models_param_dict.keys():\n",
        "    clf = model\n",
        "    clf.fit(X_train_tfidf, Ytrain)\n",
        "    result_accuracy[clf] = accuracy_score(Ytest, clf.predict(X_test_tfidf))\n",
        "\n",
        "# Print the accuracies in order of best to worst classifier\n",
        "sorted_accuracy = sorted(result_accuracy.items(), key=lambda x: x[1], reverse=True)\n",
        "for clf, accuracy in sorted_accuracy:\n",
        "    print(clf, accuracy)\n",
        "\n",
        "# sort models by accuracy in sorted_accuracy\n",
        "sorted_default_models = [mdl for (mdl, _) in sorted_accuracy]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jNHxFnCWhKHe"
      },
      "outputs": [],
      "source": [
        "# Second: Find the best hyperparameter combination for each model\n",
        "\n",
        "def find_best_hyperparameters_tfidf(id):\n",
        "  clf = list(models_param_dict.keys())[id]\n",
        "  print(f\"Train {str(clf)}!\")\n",
        "  clf_params = models_param_dict[clf]\n",
        "  best_clf, best_clf_params = training_pipeline(Xtrain, Ytrain, clf, parameters_tfidf | clf_params)\n",
        "  print(f\"{str(clf)} trained!\")\n",
        "  clf_test_accuracy = accuracy_score(Ytest, best_clf.predict(Xtest))\n",
        "  send_telegram_message(f\"{clf} [{clf_test_accuracy}]: {best_clf_params}\")\n",
        "  return (best_clf, clf_test_accuracy, best_clf_params)\n",
        "\n",
        "def find_best_hyperparameters_mpnet(id):\n",
        "  clf = list(models_param_dict.keys())[id]\n",
        "  model_name = str(clf).split('(')[0]\n",
        "  clf_params = models_param_dict[clf]\n",
        "  # we have to use the pipeline, otherwise the parameter dicts wont work\n",
        "  pipeline = Pipeline(steps=[(\"clf\", clf)])\n",
        "  # Create the RandomizedSearchCV object\n",
        "  random_search = RandomizedSearchCV(\n",
        "    pipeline,\n",
        "    param_distributions=clf_params,\n",
        "    n_iter=5, # Adjust the number of iterations based on computational resources\n",
        "    n_jobs=-1, # Use all available CPU cores\n",
        "    random_state=0\n",
        "  )\n",
        "  send_telegram_message(f\"{model_name}: start\")\n",
        "  random_search.fit(x_train_emb, Ytrain)\n",
        "  best_clf = random_search.best_estimator_\n",
        "  best_clf_params = random_search.best_params_\n",
        "  clf_test_accuracy = accuracy_score(Ytest, best_clf.predict(x_test_emb))\n",
        "  save_model(best_clf, f\"{model_name}_embd.pkl\")\n",
        "  send_telegram_message(f\"{model_name} [{clf_test_accuracy}]: {best_clf_params}\")\n",
        "  return (best_clf, clf_test_accuracy, best_clf_params)\n",
        "\n",
        "# The best models could be obtained through performing the hyperparameter search every time we run the notebook\n",
        "best_models = []\n",
        "for i in range(1, len(models_param_dict)):\n",
        "   best_models.append(find_best_hyperparameters_tfidf(i))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IjirgnBAhKHf"
      },
      "outputs": [],
      "source": [
        "# Collection of the best found hyperparameter combinations\n",
        "# Note the limitation that due to computational limitations, only one random seed was used which can lead to highly skewed results!\n",
        "best_models = [\n",
        "  (Perceptron(), 0.7984306032368809, {'tfidf__use_idf': True, 'tfidf__stop_words': None, 'tfidf__smooth_idf': True, 'tfidf__ngram_range': (1, 1), 'tfidf__max_features': None, 'tfidf__analyzer': 'word', 'clf__penalty': 'l1', 'clf__eta0': 0.1, 'clf__alpha': 1e-06}),\n",
        "  (MultinomialNB(), 0.8734673859735165, {'tfidf__use_idf': True, 'tfidf__stop_words': None, 'tfidf__smooth_idf': True, 'tfidf__ngram_range': (1, 2), 'tfidf__max_features': None, 'tfidf__analyzer': 'word', 'clf__fit_prior': False, 'clf__alpha': 1.0}),\n",
        "  (DecisionTreeClassifier(), 0.6949485041687101, {'tfidf__use_idf': False, 'tfidf__stop_words': ['perhaps', '‘d', 'are', 'that', 'toward', 'last', 'once', 'amount', 'serious', 'own', 'meanwhile', 'noone', 'thereafter', 'too', 'would', 'two', 'else', 'never', 'n‘t', 'thru', 'both', 'and', 'as', 'hereupon', 'take', 'along', 'becoming', 'becomes', 'how', 'there', 'forty', 'whom', 'fifteen', 'such', 'empty', 'those', 'hers', 'next', 'in', 'see', 'during', 'its', \"'d\", 'four', 'itself', '‘ve', 'just', 'regarding', 'whereafter', 'can', 'neither', 'these', 'ten', 'whereby', 'i', 'within', 'a', 'but', '’ve', 'make', 'since', 'therein', 'what', 'around', 'you', 'most', 'my', 'former', 'ca', '‘ll', 'because', 'beyond', 'where', 'ourselves', 'on', 'nobody', 'across', 'someone', 'somewhere', 'us', 'did', 'seemed', 'whither', 'many', 'cannot', 'being', 'ours', 'via', 'if', 'though', 'mostly', 'top', 'used', 'out', 'sometimes', 'sometime', 'than', 'except', 'fifty', 'off', 'made', 'below', 'hence', 'before', 'however', 'thus', 'doing', 'they', 'already', 'throughout', 'between', 'front', 'to', 'does', 'seems', 'nowhere', 'it', 'anyway', 'now', 'really', 'besides', 'very', 'twenty', 'his', 'wherever', 'always', 'none', 'herein', 'some', 'same', 'almost', 'an', 'well', 'often', 'whoever', 'by', 'say', 'another', 'seeming', 'everything', 'each', 'any', 'six', 'must', 'eleven', 'of', 'our', 'namely', 'due', 'eight', 'latter', 'do', 'was', 'had', 'whereupon', 'wherein', 'twelve', 'himself', 'why', '‘s', 'nothing', 'behind', 'yourselves', 'quite', 'side', 're', \"'ll\", 'please', 'afterwards', \"n't\", 'first', 'after', 'from', 'sixty', 'seem', '’d', '’re', 'anywhere', 'were', 'herself', 'more', 'several', 'who', 'will', 'towards', 'unless', 'has', 'everywhere', 'without', 'is', 'also', 'their', 'go', 'less', 'name', \"'s\", 'therefore', 'anyhow', 'over', 'bottom', 'might', 'thence', 'every', 'latterly', 'together', 'n’t', 'not', 'nor', 'anyone', 'full', 'move', 'other', 'about', 'he', 'thereupon', 'up', 'for', 'no', 'somehow', 'using', 'enough', 'others', 'this', 'third', 'least', 'beforehand', 'so', 'her', 'may', 'the', 'show', 'him', 'all', 'themselves', 'have', \"'m\", 'elsewhere', 'when', 'mine', 'rather', 'either', '’s', 'whenever', 'ever', 'moreover', 'yours', 'hundred', 'be', 'should', 'indeed', 'keep', 'until', 'them', 'put', 'still', 'much', 'whole', 'again', 'otherwise', 'through', 'various', \"'ve\", 'into', 'whence', 'became', 'hereafter', 'back', 'here', 'something', 'everyone', 'myself', 'formerly', 'become', 'she', 'under', 'few', 'per', 'whose', 'further', '‘re', 'five', 'am', 'hereby', 'among', 'onto', 'yourself', 'only', 'could', 'nine', 'we', 'amongst', 'yet', 'whereas', 'which', 'with', 'your', '’ll', '’m', 'while', \"'re\", 'against', 'whatever', 'down', 'beside', 'thereby', '‘m', 'above', 'alone', 'get', 'one', 'nevertheless', 'although', 'give', 'call', 'part', 'then', 'even', 'three', 'or', 'whether', 'anything', 'me', 'done', 'upon', 'been', 'at'], 'tfidf__smooth_idf': True, 'tfidf__ngram_range': (1, 1), 'tfidf__max_features': 1000, 'tfidf__analyzer': 'word', 'clf__min_samples_split': 5, 'clf__min_samples_leaf': 1, 'clf__max_features': 'log2', 'clf__max_depth': 110}),\n",
        "  (DummyClassifier(), 0.49975478175576266, {'tfidf__use_idf': False, 'tfidf__stop_words': None, 'tfidf__smooth_idf': False, 'tfidf__ngram_range': (1, 2), 'tfidf__max_features': None, 'tfidf__analyzer': 'char'}),\n",
        "  (LinearSVC(), 0.8847474252084355, {'tfidf__use_idf': False, 'tfidf__stop_words': None, 'tfidf__smooth_idf': True, 'tfidf__ngram_range': (1, 2), 'tfidf__max_features': None, 'tfidf__analyzer': 'word', 'clf__tol': 0.0001, 'clf__loss': 'squared_hinge', 'clf__C': 10}),\n",
        "  (GradientBoostingClassifier(), 0.7827366356056891, {'tfidf__use_idf': True, 'tfidf__stop_words': None, 'tfidf__smooth_idf': True, 'tfidf__ngram_range': (1, 2), 'tfidf__max_features': 1000, 'tfidf__analyzer': 'word', 'clf__subsample': 0.8, 'clf__n_estimators': 500, 'clf__max_features': 'log2', 'clf__max_depth': 5, 'clf__learning_rate': 0.01}),\n",
        "  (LogisticRegression(), 0.8916135360470819, {'tfidf__use_idf': True, 'tfidf__stop_words': None, 'tfidf__smooth_idf': False, 'tfidf__ngram_range': (1, 2), 'tfidf__max_features': None, 'tfidf__analyzer': 'word', 'clf__solver': 'saga', 'clf__penalty': 'l1', 'clf__C': 100}),\n",
        "  (KNeighborsClassifier(), 0.8263854830799412, {'tfidf__use_idf': True, 'tfidf__stop_words': None, 'tfidf__smooth_idf': True, 'tfidf__ngram_range': (1, 1), 'tfidf__max_features': None, 'tfidf__analyzer': 'word', 'clf__weights': 'distance', 'clf__n_neighbors': 26, 'clf__algorithm': 'auto'}),\n",
        "  (RandomForestClassifier(), 0.784698381559588, {'tfidf__use_idf': False, 'tfidf__stop_words': ['them', '’m', 'everywhere', 'two', 'such', 'however', 'me', 'no', 'make', 'what', 'various', 'see', 'per', 'made', 'top', 'least', 'front', 'mine', 'four', \"'s\", 'afterwards', 'not', 'until', 'do', 'get', 'beyond', 'both', 'side', 'whether', 'might', 'above', 'we', 'here', 'the', 'themselves', 'had', 'across', 'too', 'through', 'anywhere', 'wherein', 'eight', \"n't\", 'twelve', 'may', 'yours', 'seemed', 'take', 'he', 'among', 'besides', 'really', 'your', 'and', 'almost', 'how', 'seems', 'go', 'during', 'either', 'my', 'whither', 'each', 'moreover', 'empty', 'part', 'cannot', 'someone', 'call', 'even', 'onto', 'seeming', 'fifteen', 'last', 'throughout', 'ever', 'she', '’re', '‘m', 'six', 'already', '‘d', 'full', 'under', 'whence', 'quite', 'enough', 'around', 'somewhere', 'sixty', 'there', 'or', 'then', 'nowhere', 'keep', 'its', 'also', 'up', 'but', 'hereafter', 'yourself', 'against', 'five', 'some', 'therefore', 'sometimes', 'done', 'into', 'over', '‘ve', 'these', 'now', 'neither', 'mostly', 'does', 'further', 'beside', 'thereafter', 'rather', 'hers', 'something', 'did', 'ca', 'same', 'her', 'ten', 'noone', '’ve', 'for', 'forty', 'used', 'whole', 'few', 'from', 'with', 'whose', \"'ve\", 'below', 'elsewhere', 'latterly', 'him', 'on', 'first', '‘s', 'yet', 'where', 'anything', 'so', 'their', 'about', 'several', 'must', 'amount', 'everyone', 'those', 'this', 'thus', 'was', 'could', 'being', 'though', 'at', 'amongst', 'formerly', '’d', 'because', \"'d\", 'while', 'out', 'one', 'after', 'serious', 'should', 'hereupon', 'were', 'often', '’s', 'hence', 'without', 'much', 'an', 'n’t', 'nor', 'regarding', 'never', 'a', 'become', 'latter', 'behind', '’ll', '‘ll', 'whenever', 'fifty', 'within', 'wherever', 'seem', 'three', 'thereby', 'down', 'became', 'nevertheless', 'once', 'his', 'herein', \"'ll\", 'it', 'will', 'whereas', 'himself', 'ourselves', 'together', 'herself', 'would', 'show', 'give', 'to', 'nine', 'thence', 'whom', 'be', 'next', 'toward', 'can', 'just', 'namely', 'bottom', 'ours', 'every', 'please', 'twenty', 'otherwise', 'along', 'move', 'whereafter', 'although', 'sometime', 'off', 'only', \"'m\", 'say', 're', 'thru', 'whereby', 'using', 'if', 'again', 'most', 'whereupon', 'am', 'except', 'which', 'you', 'own', 'former', 'unless', 'becomes', 'less', 'anyway', 'due', 'another', 'they', 'third', 'everything', 'via', 'always', 'has', 'hereby', 'back', 'more', 'any', 'whatever', 'whoever', 'why', 'somehow', 'put', 'name', 'before', 'is', 'our', 'upon', \"'re\", 'as', 'doing', 'beforehand', 'meanwhile', 'when', 'nobody', 'others', 'else', 'all', 'i', 'towards', 'other', 'since', 'in', 'yourselves', 'indeed', 'anyone', 'us', 'n‘t', 'of', 'hundred', 'still', 'none', 'becoming', 'between', 'itself', 'anyhow', 'than', 'by', 'thereupon', 'been', 'perhaps', 'myself', 'well', 'eleven', 'alone', '‘re', 'nothing', 'very', 'therein', 'have', 'that', 'who', 'are', 'many'], 'tfidf__smooth_idf': True, 'tfidf__ngram_range': (1, 1), 'tfidf__max_features': None, 'tfidf__analyzer': 'word', 'clf__n_estimators': 1200, 'clf__min_samples_split': 5, 'clf__min_samples_leaf': 1, 'clf__max_features': 'log2', 'clf__max_depth': 50, 'clf__bootstrap': False}),\n",
        "]\n",
        "\n",
        "# Define the best performing classifiers\n",
        "best_models = sorted(best_models, key=lambda x: x[1], reverse=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6NaR1O1hKHf"
      },
      "source": [
        "### Generating confusion matrices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RFJDxq2UhKHf"
      },
      "outputs": [],
      "source": [
        "# Confusion matrices\n",
        "def generate_confusion_matrices(id):\n",
        "    best_model = best_models[id]\n",
        "    clf = best_model[0]\n",
        "    model_name = str(clf).split('(')[0]\n",
        "    clf_score = best_model[1]\n",
        "    clf_params = best_model[2]\n",
        "    pipe = get_model_pipeline(clf)\n",
        "    pipe.set_params(**clf_params)\n",
        "    pipe.fit(Xtrain, Ytrain)\n",
        "    save_model(pipe, f\"{model_name}.pkl\")\n",
        "    Ypred = pipe.predict(Xtest)\n",
        "    cm = confusion_matrix(Ytest, Ypred)\n",
        "    plt.figure(figsize=(6,6))\n",
        "    sns.heatmap(cm, annot=True, fmt=\".3f\", linewidths=.5, square=True, cmap='OrRd', xticklabels=['anti-vaccination', 'pro-vaccination'], yticklabels=['anti-vaccination', 'pro-vaccination'])\n",
        "    plt.ylabel('Actual label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    all_sample_title = f'{model_name}\\nAccuracy Score: {clf_score:.4f}'\n",
        "    plt.title(all_sample_title, size=15)\n",
        "    plt.show()\n",
        "    plt.savefig(Path(f\"./{model_name}.pdf\"), bbox_inches='tight')\n",
        "\n",
        "    # Display a few samples that were wrongly classified\n",
        "    misclassified_samples = Xtest[Ytest != Ypred]\n",
        "    misclassified_labels = Ytest[Ytest != Ypred]\n",
        "    with open(Path(f\"./{model_name}.txt\"), \"w+\") as f:\n",
        "      for sample, label in zip(misclassified_samples[:10], misclassified_labels[:10]):\n",
        "          s = f\"Sample: {sample}\\nClassified as: {label_dict[int(not(label))]}\\nTrue Label: {label_dict[label]}\\n\\n\"\n",
        "          print(s)\n",
        "          f.write(s)\n",
        "\n",
        "\n",
        "for i in range(1, len(models_param_dict)):\n",
        "  generate_confusion_matrices(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aqEVI3SjhKHg"
      },
      "outputs": [],
      "source": [
        "# When performing the hyperparameter search on the RWTH Aachen High Performance Computing cluster, we submit a SLURM Array Job and use the task ID as a model identifier\n",
        "# Note that we would use a Python script, not a notebook. Thus, also some part of this notebook might seem a bit unorganized and cluttered. But as a matter of fact, we never used one single notebook to run our code because it was infeasible to do so.\n",
        "if __name__ == \"__main__\":\n",
        "  args = parse_arguments()\n",
        "  _ = find_best_hyperparameters_tfidf(args.model_id)\n",
        "  generate_confusion_matrices(args.model_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extract important features"
      ],
      "metadata": {
        "id": "2s3VrizVh78r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = Pipeline(steps = [(\"tfidf\",\n",
        "    TfidfVectorizer(\n",
        "        use_idf=False,\n",
        "        stop_words=['perhaps', '‘d', 'are', 'that', 'toward', 'last', 'once', 'amount', 'serious', 'own', 'meanwhile', 'noone', 'thereafter', 'too', 'would', 'two', 'else', 'never', 'n‘t', 'thru', 'both', 'and', 'as', 'hereupon', 'take', 'along', 'becoming', 'becomes', 'how', 'there', 'forty', 'whom', 'fifteen', 'such', 'empty', 'those', 'hers', 'next', 'in', 'see', 'during', 'its', \"'d\", 'four', 'itself', '‘ve', 'just', 'regarding', 'whereafter', 'can', 'neither', 'these', 'ten', 'whereby', 'i', 'within', 'a', 'but', '’ve', 'make', 'since', 'therein', 'what', 'around', 'you', 'most', 'my', 'former', 'ca', '‘ll', 'because', 'beyond', 'where', 'ourselves', 'on', 'nobody', 'across', 'someone', 'somewhere', 'us', 'did', 'seemed', 'whither', 'many', 'cannot', 'being', 'ours', 'via', 'if', 'though', 'mostly', 'top', 'used', 'out', 'sometimes', 'sometime', 'than', 'except', 'fifty', 'off', 'made', 'below', 'hence', 'before', 'however', 'thus', 'doing', 'they', 'already', 'throughout', 'between', 'front', 'to', 'does', 'seems', 'nowhere', 'it', 'anyway', 'now', 'really', 'besides', 'very', 'twenty', 'his', 'wherever', 'always', 'none', 'herein', 'some', 'same', 'almost', 'an', 'well', 'often', 'whoever', 'by', 'say', 'another', 'seeming', 'everything', 'each', 'any', 'six', 'must', 'eleven', 'of', 'our', 'namely', 'due', 'eight', 'latter', 'do', 'was', 'had', 'whereupon', 'wherein', 'twelve', 'himself', 'why', '‘s', 'nothing', 'behind', 'yourselves', 'quite', 'side', 're', \"'ll\", 'please', 'afterwards', \"n't\", 'first', 'after', 'from', 'sixty', 'seem', '’d', '’re', 'anywhere', 'were', 'herself', 'more', 'several', 'who', 'will', 'towards', 'unless', 'has', 'everywhere', 'without', 'is', 'also', 'their', 'go', 'less', 'name', \"'s\", 'therefore', 'anyhow', 'over', 'bottom', 'might', 'thence', 'every', 'latterly', 'together', 'n’t', 'not', 'nor', 'anyone', 'full', 'move', 'other', 'about', 'he', 'thereupon', 'up', 'for', 'no', 'somehow', 'using', 'enough', 'others', 'this', 'third', 'least', 'beforehand', 'so', 'her', 'may', 'the', 'show', 'him', 'all', 'themselves', 'have', \"'m\", 'elsewhere', 'when', 'mine', 'rather', 'either', '’s', 'whenever', 'ever', 'moreover', 'yours', 'hundred', 'be', 'should', 'indeed', 'keep', 'until', 'them', 'put', 'still', 'much', 'whole', 'again', 'otherwise', 'through', 'various', \"'ve\", 'into', 'whence', 'became', 'hereafter', 'back', 'here', 'something', 'everyone', 'myself', 'formerly', 'become', 'she', 'under', 'few', 'per', 'whose', 'further', '‘re', 'five', 'am', 'hereby', 'among', 'onto', 'yourself', 'only', 'could', 'nine', 'we', 'amongst', 'yet', 'whereas', 'which', 'with', 'your', '’ll', '’m', 'while', \"'re\", 'against', 'whatever', 'down', 'beside', 'thereby', '‘m', 'above', 'alone', 'get', 'one', 'nevertheless', 'although', 'give', 'call', 'part', 'then', 'even', 'three', 'or', 'whether', 'anything', 'me', 'done', 'upon', 'been', 'at'],\n",
        "        smooth_idf=True,\n",
        "        ngram_range=(1, 1),\n",
        "        max_features=1000,\n",
        "        analyzer='word'\n",
        "    )), (\"clf\",\n",
        "    DecisionTreeClassifier(\n",
        "        min_samples_split=5,\n",
        "        min_samples_leaf=1,\n",
        "        max_features='log2',\n",
        "        max_depth=110\n",
        "    ))])\n",
        "\n",
        "\n",
        "pipeline.fit(Xtrain, Ytrain)\n",
        "\n",
        "feature_names = pipeline.named_steps['tfidf'].get_feature_names_out()\n",
        "feature_importances = pipeline.named_steps['clf'].feature_importances_\n",
        "feature_importance_dict = {feature_names[i]: feature_importances[i] for i in range(len(feature_names))}\n",
        "top_ten_importances = sorted(feature_importance_dict.items(), key=lambda x: x[1], reverse=True)[:10]\n",
        "for feature, importance in top_ten_importances:\n",
        "    print(f\"Feature: {feature}, Importance: {importance}\")"
      ],
      "metadata": {
        "id": "__Bz_yUoiLSW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = Pipeline(steps = [(\"tfidf\",\n",
        "    TfidfVectorizer(\n",
        "        use_idf=True,\n",
        "        stop_words=None,\n",
        "        smooth_idf=False,\n",
        "        ngram_range=(1, 2),\n",
        "        max_features=None,\n",
        "        analyzer='word'\n",
        "    )),  (\"clf\",\n",
        "    LogisticRegression(\n",
        "        solver='saga',\n",
        "        penalty='l1',\n",
        "        C=100\n",
        "    ))])\n",
        "\n",
        "pipeline.fit(Xtrain, Ytrain)\n",
        "\n",
        "feature_names = pipeline.named_steps['tfidf'].get_feature_names_out()\n",
        "\n",
        "coefficients_class_1 = pipeline.named_steps['clf'].coef_[0]\n",
        "coefficients_class_0 = -coefficients_class_1\n",
        "\n",
        "# Ten highest weights and corresponding words for class 1\n",
        "top_ten_weights_class_1 = sorted(zip(feature_names, coefficients_class_1), key=lambda x: x[1], reverse=True)[:10]\n",
        "\n",
        "# Ten highest weights and corresponding words for class 0\n",
        "top_ten_weights_class_0 = sorted(zip(feature_names, coefficients_class_0), key=lambda x: x[1], reverse=True)[:10]\n",
        "\n",
        "print(\"Top features for Class 1:\")\n",
        "for feature, weight in top_ten_weights_class_1:\n",
        "    print(f\"Feature: {feature}, Weight: {weight}\")\n",
        "\n",
        "print(\"\\nTop features for Class 0:\")\n",
        "for feature, weight in top_ten_weights_class_0:\n",
        "    print(f\"Feature: {feature}, Weight: {weight}\")\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "_r-C_YiNrTSR"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}